{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Vtran4/M2C12-Inheritance/blob/master/Conversational_AI_Module_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IUcYmWStoe6r"
      },
      "source": [
        "# Wordclouds\n",
        "\n",
        "Wordclouds are excellent ways to summarize textual information like reviews, customer feedback, documents etc. The first part of this excercise focuses on creating a word cloud from the text descriptions in the wine dataset that you have seen earlier in Part 1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "riY1Eyuroe6s"
      },
      "source": [
        "!pip install wordcloud"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hY2_qhuXoe6s"
      },
      "source": [
        "# Start with loading all necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from os import path\n",
        "from PIL import Image\n",
        "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
        "from nltk import FreqDist\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from nltk.tokenize import RegexpTokenizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "enB5dFbRQRP0"
      },
      "source": [
        "**Load the Data files**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5cSb71oLOsu_"
      },
      "source": [
        "! git clone https://github.com/vibsabhishek/EP290.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "etfksvnooe6s"
      },
      "source": [
        "# Load in the dataframe\n",
        "df = pd.read_csv(\"EP290/winemag-data-2500.csv\", index_col=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pXqh0JvVoe6u"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d7_vI_lpzq2b"
      },
      "source": [
        "print(df.description[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Ug0zRHaoe6t"
      },
      "source": [
        "# Start with one review:\n",
        "text = df.description[0]\n",
        "\n",
        "# Create and generate a word cloud image:\n",
        "wordcloud = WordCloud().generate(text)\n",
        "\n",
        "# Display the generated image:\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7XW-A1Rqoe6t"
      },
      "source": [
        "print(text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3DNGx-booe6u"
      },
      "source": [
        "# Save the image in the img folder:\n",
        "wordcloud.to_file(\"first_review.png\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "88aXBP8Poe6u"
      },
      "source": [
        "#combine all the descriptions into one big text variable\n",
        "text = \" \".join(description for description in df.description)\n",
        "print (\"There are {} words in the combination of all review.\".format(len(text)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N0mjME-Voe6v"
      },
      "source": [
        "tokenizer = RegexpTokenizer(r'\\w+')\n",
        "tokens = tokenizer.tokenize(text)\n",
        "freq = FreqDist(tokens)\n",
        "freq.plot(50)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BWzp4CzG3Ek4"
      },
      "source": [
        "plt.rcParams[\"figure.figsize\"] = (10,10)\n",
        "freq.plot(50)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ltdr0knVoe6v"
      },
      "source": [
        "## Q1. Plot the distribution of the entire corpus"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NZzoSXVO1EJN"
      },
      "source": [
        "freq.plot()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sbPx1EkXoe6w"
      },
      "source": [
        "## Zipf's law\n",
        "\n",
        "The distribution of text follows a Zipf's or scale-free distribution. This is quite characteristic of any naturally occuring text corpus, irrespective of language.\n",
        "\n",
        "## Q2. What is the most commonly occuring word?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dQQrsBk6oe6w"
      },
      "source": [
        "### Stopwords\n",
        "\n",
        "Some words like the, and etc. even though commonly occuring do not add a lot of value as they are not unique to the context. In addition, we might wish to remove commonly occuring words for a specific context, e.g. \"wine\", and \"drink\"."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OsMopeFfoe6w"
      },
      "source": [
        "# Create stopword list:\n",
        "stopwords = set(STOPWORDS)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EWvPszusoe6w"
      },
      "source": [
        "## Q3. Add a set of stopwords specific to the wine descriptions and generate another wordcloud"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DD5skiIOoe6w"
      },
      "source": [
        "stopwords.update([\"wine\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ysnaF2dQoe6x"
      },
      "source": [
        "# Generate a word cloud image\n",
        "wordcloud = WordCloud(stopwords=stopwords, background_color=\"white\", width=1000, height=800).generate(text)\n",
        "\n",
        "# Display the generated image:\n",
        "# the matplotlib way:\n",
        "plt.rcParams[\"figure.figsize\"] = (20,20)\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HGpuIgG53nza"
      },
      "source": [
        "plt.rcParams[\"figure.figsize\"] = (20,20)\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xMGaSPIcoe6x"
      },
      "source": [
        "## Apply an image mask"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QEZ7HFiAoe6x"
      },
      "source": [
        "wine_mask = np.array(Image.open(\"EP290/winemask.png\"))\n",
        "wine_mask"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zsfVSTsIoe6x"
      },
      "source": [
        "def transform_format(val):\n",
        "    if val == 0:\n",
        "        return 255\n",
        "    else:\n",
        "        return val\n",
        "\n",
        "# Transform your mask into a new one that will work with the function:\n",
        "transformed_wine_mask = np.ndarray((wine_mask.shape[0],wine_mask.shape[1]), np.int32)\n",
        "\n",
        "for i in range(len(wine_mask)):\n",
        "    transformed_wine_mask[i] = list(map(transform_format, wine_mask[i]))\n",
        "\n",
        "# Check the expected result of your mask\n",
        "transformed_wine_mask\n",
        "\n",
        "# Create a word cloud image\n",
        "wc = WordCloud(background_color=\"white\", mask=transformed_wine_mask,\n",
        "               stopwords=stopwords, contour_width=1, contour_color='firebrick', width=1000, height=800)\n",
        "\n",
        "# Generate a wordcloud\n",
        "wc.generate(text)\n",
        "\n",
        "# store to file\n",
        "wc.to_file(\"wine.png\")\n",
        "\n",
        "# show\n",
        "plt.figure(figsize=[10,10])\n",
        "plt.imshow(wc)\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BVu9fl8vfq39"
      },
      "source": [
        "## Q4. Create the world cloud for the abstracts in COVID data (in file: EP290/covid19_small.csv)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gfy_lSDOFgSO"
      },
      "source": [
        "df = pd.read_csv(\"EP290/covid19_small.csv\", index_col=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "USR7e1znFpuk"
      },
      "source": [
        "df.abstract[1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bdR0JFD5oe6y"
      },
      "source": [
        "\n",
        "# Sentiment analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ncesuhQvoe6y"
      },
      "source": [
        "Here we will try to find the sentiment of various text, a very common application of NLP."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "siFNCyoOoe6y"
      },
      "source": [
        "## Import packages\n",
        "Make sure you installed ***sklearn***, ***matplotlib*** and ***numpy*** if you use your local machine"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zI0hUnLJKPKS",
        "outputId": "479207d1-71f4-4100-ac5c-08aba1b728c2"
      },
      "source": [
        "!pip install -U textblob\n",
        "!pip install vaderSentiment\n",
        "!python -m textblob.download_corpora"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: textblob in /usr/local/lib/python3.7/dist-packages (0.15.3)\n",
            "Collecting textblob\n",
            "  Downloading textblob-0.17.1-py2.py3-none-any.whl (636 kB)\n",
            "\u001b[K     |████████████████████████████████| 636 kB 7.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: nltk>=3.1 in /usr/local/lib/python3.7/dist-packages (from textblob) (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk>=3.1->textblob) (1.15.0)\n",
            "Installing collected packages: textblob\n",
            "  Attempting uninstall: textblob\n",
            "    Found existing installation: textblob 0.15.3\n",
            "    Uninstalling textblob-0.15.3:\n",
            "      Successfully uninstalled textblob-0.15.3\n",
            "Successfully installed textblob-0.17.1\n",
            "Collecting vaderSentiment\n",
            "  Downloading vaderSentiment-3.3.2-py2.py3-none-any.whl (125 kB)\n",
            "\u001b[K     |████████████████████████████████| 125 kB 9.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from vaderSentiment) (2.23.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->vaderSentiment) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->vaderSentiment) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->vaderSentiment) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->vaderSentiment) (1.24.3)\n",
            "Installing collected packages: vaderSentiment\n",
            "Successfully installed vaderSentiment-3.3.2\n",
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/brown.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package conll2000 to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/conll2000.zip.\n",
            "[nltk_data] Downloading package movie_reviews to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/movie_reviews.zip.\n",
            "Finished.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SlzuxK0_oe6y"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from os import path\n",
        "import seaborn as sns\n",
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
        "from sklearn.metrics import confusion_matrix, precision_score, precision_recall_curve, recall_score, f1_score, accuracy_score\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EB0z8tpkJ_QP"
      },
      "source": [
        "##**Sentiment Analysis using VADER**\n",
        "\n",
        "*   For compound > 0.05 - Positive\n",
        "*   For compound < -0.05 - Negative\n",
        "*   else Neutral\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xYITCOeFJ9c3",
        "outputId": "18e2194d-56aa-4eb2-8d87-33bbf40980de"
      },
      "source": [
        "vader = SentimentIntensityAnalyzer()\n",
        "text_sentiment = vader.polarity_scores(\"VADER is amazingly simple to use. What great fun!\")\n",
        "print(text_sentiment)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'neg': 0.0, 'neu': 0.476, 'pos': 0.524, 'compound': 0.8268}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AvyjptyYKow_",
        "outputId": "d1da7e72-8439-4e12-ffec-7a012ad677ba"
      },
      "source": [
        "text_sentiment = vader.polarity_scores(\"VADER is terrible to use. What a shame!\")\n",
        "print(text_sentiment)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'neg': 0.52, 'neu': 0.48, 'pos': 0.0, 'compound': -0.7574}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Btr7W9J06BGr",
        "outputId": "717904a8-3023-4c7f-e9b0-34dd9e55fb9c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "text_sentiment = vader.polarity_scores(\"He is not going to hate that.\")\n",
        "print(text_sentiment)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'neg': 0.0, 'neu': 0.667, 'pos': 0.333, 'compound': 0.4585}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v7E761dfWOjx"
      },
      "source": [
        "## **Sentiment analysis on IMDB review dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X275PN0QVGwG",
        "outputId": "501dd266-3a51-4f5f-a5c8-7135a3232186",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        }
      },
      "source": [
        "df = pd.read_excel(\"EP290/IMDB_Dataset_small.xls\", header=0)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-2d8dfc0d0363>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_excel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"EP290/IMDB_Dataset_small.xls\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/excel/_base.py\u001b[0m in \u001b[0;36mread_excel\u001b[0;34m(io, sheet_name, header, names, index_col, usecols, squeeze, dtype, engine, converters, true_values, false_values, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, parse_dates, date_parser, thousands, comment, skipfooter, convert_float, mangle_dupe_cols, storage_options)\u001b[0m\n\u001b[1;32m    362\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExcelFile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    363\u001b[0m         \u001b[0mshould_close\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 364\u001b[0;31m         \u001b[0mio\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mExcelFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    365\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    366\u001b[0m         raise ValueError(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/excel/_base.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, path_or_buffer, engine, storage_options)\u001b[0m\n\u001b[1;32m   1190\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1191\u001b[0m                 ext = inspect_excel_format(\n\u001b[0;32m-> 1192\u001b[0;31m                     \u001b[0mcontent_or_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1193\u001b[0m                 )\n\u001b[1;32m   1194\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mext\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/excel/_base.py\u001b[0m in \u001b[0;36minspect_excel_format\u001b[0;34m(content_or_path, storage_options)\u001b[0m\n\u001b[1;32m   1069\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1070\u001b[0m     with get_handle(\n\u001b[0;32m-> 1071\u001b[0;31m         \u001b[0mcontent_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_text\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1072\u001b[0m     ) as handle:\n\u001b[1;32m   1073\u001b[0m         \u001b[0mstream\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    709\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    710\u001b[0m             \u001b[0;31m# Binary mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 711\u001b[0;31m             \u001b[0mhandle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    712\u001b[0m         \u001b[0mhandles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    713\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'EP290/IMDB_Dataset_small.xls'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k6aYZJL5VV3j"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1cbJuV3L7Ch7"
      },
      "source": [
        "len(df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Mf6ArKuict8"
      },
      "source": [
        "## Q5. Show the distribution of words containted in the IMDB reviews."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x9LWmH0rMtHG"
      },
      "source": [
        "#some helper function to measure sentiment\n",
        "def detect_vader_pos(text):\n",
        "    return vader.polarity_scores(text)['pos']\n",
        "\n",
        "def detect_vader_neg(text):\n",
        "    return vader.polarity_scores(text)['neg']\n",
        "\n",
        "def detect_vader_comp(text):\n",
        "    return vader.polarity_scores(text)['compound']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hz4Qlz2lVe2P"
      },
      "source": [
        "print(\"Review:\", df.review[0],\"\\n Sentiment:\", df.sentiment[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9_I5X_8-Wu9H"
      },
      "source": [
        "###Compute Sentiment for the entire dataset using VADER"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "irJT51_mV6HZ"
      },
      "source": [
        "vader = SentimentIntensityAnalyzer()\n",
        "df['vader_pos'] = df.review.apply(detect_vader_pos)\n",
        "df['vader_neg'] = df.review.apply(detect_vader_neg)\n",
        "df['vader_comp'] = df.review.apply(detect_vader_comp)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NFqKvoeXW2gR"
      },
      "source": [
        "#Visualize the results\n",
        "plt.figure(figsize=[4,4])\n",
        "\n",
        "ax = sns.violinplot(x=\"sentiment\", y=\"vader_comp\", data=df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DqijOfqxXTGY"
      },
      "source": [
        "plt.figure(figsize=[4,4])\n",
        "ax = sns.violinplot(x=\"sentiment\", y=\"vader_pos\", data=df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kUb0wUh4XblX"
      },
      "source": [
        "plt.figure(figsize=[4,4])\n",
        "ax = sns.violinplot(x=\"sentiment\", y=\"vader_neg\", data=df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FM7DoGrdXtwt"
      },
      "source": [
        "v_pred = np.where(df['vader_comp'] > 0.0, \"positive\", \"negative\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Lq7fugqXy5k"
      },
      "source": [
        "print(\"Confusion Matrix:\\n\", confusion_matrix(df.sentiment, v_pred))\n",
        "print(\"F1 score:\", f1_score(df.sentiment, v_pred, average='micro'))\n",
        "print(\"Accuracy score:\", accuracy_score(df.sentiment, v_pred))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EpaPLLAsiom0"
      },
      "source": [
        "## **Creating our own classifier**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mipFS3jEoe62"
      },
      "source": [
        "## Count Vectorizer\n",
        "- run count vectorizer on it\n",
        "- plot histograms of counts etc.\n",
        "- vary the parameters of Cvectorizer, show how histograms change"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HcP-PIT7oe62"
      },
      "source": [
        "### Split into train and test datasets\n",
        "Here, 70% of the original data are used for training models, and the rest are for test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cw7hMrAzoe63"
      },
      "source": [
        "train_sample = int(len(df)*0.7)\n",
        "train = df[0:(train_sample)]\n",
        "test = df[(train_sample+1):len(df)]\n",
        "print('train data size:', len(train))\n",
        "print('test data size:', len(test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZyRi565toe63"
      },
      "source": [
        "### Create the vector representation of training and testing data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eJ55dKvxoe63"
      },
      "source": [
        "#Encode documents\n",
        "vectorizer = CountVectorizer(stop_words='english', lowercase=True, min_df=5);\n",
        "vectorizer.fit(train.review);\n",
        "\n",
        "#create vector representation\n",
        "train_vec = vectorizer.transform(train.review)\n",
        "test_vec = vectorizer.transform(test.review)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qmynFyyd_nAk"
      },
      "source": [
        "print(train.review[2])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C80ZrgSFdXPS"
      },
      "source": [
        "print(train_vec[2])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jRK-3hh8ZLdj"
      },
      "source": [
        "lr_model = LogisticRegression(C=0.1)\n",
        "lr_model.fit(train_vec, train.sentiment)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rpvbDW9PZhlQ"
      },
      "source": [
        "lr_pred = lr_model.predict(test_vec)\n",
        "\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(test.sentiment, lr_pred))\n",
        "print(\"F1 score:\", f1_score(test.sentiment, lr_pred, average='micro'))\n",
        "print(\"Accuracy:\", accuracy_score(test.sentiment, lr_pred))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kJsZ5PySixSm"
      },
      "source": [
        "#Test an example\n",
        "reviews = [\n",
        "           \"Star Wars! This was the worst movie ever. Total waste of time.\",\n",
        "           \"Star Trek! This was the best movie ever. Totally recommended.\"\n",
        "]\n",
        "reviews_vec = vectorizer.transform(reviews)\n",
        "lr_model.predict(reviews_vec)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pVVQgFlZcqR5"
      },
      "source": [
        "## Model analysis: Examine which features are important for positive versus negative"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "901DoNj5Z1ms"
      },
      "source": [
        "!pip install eli5\n",
        "!pip install tabulate\n",
        "!pip install spacy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i7qXyfRDZvvf"
      },
      "source": [
        "import eli5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RGZ38d30aJ_v"
      },
      "source": [
        "eli5.show_weights(lr_model, top=20, vec=vectorizer)\n",
        "#eli5.show_prediction(lr_pred, reviews)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nx7wY_kpDTsv"
      },
      "source": [
        "eli5.show_prediction(lr_model, df.review[0], vec=vectorizer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zQvfofHvn3CD"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BcRBPwodmbrx"
      },
      "source": [
        "!pip install pyfiglet"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qbEIKR2fm6CS"
      },
      "source": [
        "import pyfiglet\n",
        "\n",
        "result = pyfiglet.figlet_format(\"That's all folks!!\\n\\n THANK YOU \\nfor be being such a sport with the Python notebooks!\")\n",
        "print(result)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}